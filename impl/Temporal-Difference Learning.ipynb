{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import ForbiddenAreaCfg, RewardCfg, make_gridworld\n",
    "\n",
    "env = make_gridworld(\n",
    "    forbidden_area_cfg=ForbiddenAreaCfg(num=6),\n",
    "    reward_cfg=RewardCfg(forbidden_area=-10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD learning of state value\n",
    "\n",
    "## State Value \n",
    "\n",
    "$v_{\\pi}(s_t) = \\mathbb{E}[G_t|S_t=s_t]$\n",
    "\n",
    "or\n",
    "\n",
    "$v_{\\pi}(s_t) = \\mathbb{E}[r_{t+1} + \\gamma v_{\\pi}(s_{t+1})]$\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "We aim to estimate the state value of a given policy $\\pi$.\n",
    "\n",
    "Consider this formula:\n",
    "\n",
    "$g(v_{\\pi}(s_t)) = v_{\\pi}(s_t) - \\mathbb{E}[r_{t+1} + \\gamma v_{\\pi}(s_{t+1})|s_t]$\n",
    "\n",
    "We can use Robbins-Monro algorithm to solve $g_t(v_{\\pi}(s_t)) = 0$.\n",
    "\n",
    "Then we have TD learning of state value algorithm:\n",
    "\n",
    "$v_{t+1}(s_{t}) = v_{t}(s_t) - \\alpha_t(s_t)[v_t(s_t) - [r_{t+1} + \\gamma v_t(s_{t+1})]]$\n",
    "\n",
    "This algorithm is driven by the temporal difference error and makes the state value estimate closer to the target value $\\bar{v_t}$ ($\\bar{v_t}$ is not the mean of $v_t$, it is an ideal value $v_{\\pi}$).\n",
    "\n",
    "\n",
    "## Properties\n",
    "\n",
    "- It **only** estimates the state value of a given policy.\n",
    "- It does not estimate the action value.\n",
    "- It does not search for optimal policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD learning of action value: Sarsa\n",
    "\n",
    "## Action Value\n",
    "\n",
    "$q_{\\pi}(s_t, a_t) = \\mathbb{E}[G_t|S_t=s_t, A_t=a_t]$\n",
    "\n",
    "or\n",
    "\n",
    "$q_{\\pi}(s_t, a_t) = \\mathbb{E}[r_{t+1} + \\gamma v_{\\pi}(s_{t+1})]$\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "Suppose we have some experience/data $\\{(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})\\}$.\n",
    "\n",
    "We can use the following formula to estimate the action value:\n",
    "\n",
    "$q_{t+1} = q_{t} - \\alpha_t(s_t, a_t)[q_t(s_t, a_t) - [r_{t+1} + \\gamma q_t(s_{t+1}, a_{t+1})]]$\n",
    "\n",
    "> The above formula is to solve $g(q_{\\pi}) = 0$,\n",
    ">\n",
    "> where $g(q_{\\pi}(a_t, s_t)) = q_{\\pi}(a_t, s_t) - \\mathbb{E}[r_{t+1} + \\gamma q_{\\pi}(a_{t+1}, s_{t+1})|s_t, a_t]$\n",
    "\n",
    "This formula is used to solve the Bellman equation in terms of action values:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &= \\sum_r rp(r|s, a) + \\gamma\\sum_{s'} \\sum_{a'}q_{\\pi}(s', a')p(s'|s, a)\\pi(a'|s')\\\\\n",
    "&= \\sum_r rp(r|s, a) + \\gamma \\sum_{s'} p(s'|s, a) \\sum_{a'}q_{\\pi}(s', a')\\pi(a'|s')\\\\\\\n",
    "&= \\sum_r rp(r|s, a) + \\gamma \\sum_{s'} \\sum_{a'} q_{\\pi}(s', a') p(s', a'|s, a)\\\\\n",
    "&= \\mathbb{E}[r + \\gamma q_{\\pi}(s', a')|s, a]\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(s', a'|s, a)\n",
    "&= p(s'|s, a)p(a'|s', s, a)\\\\\n",
    "&= p(s'|s, a)p(a'|s')\\\\\n",
    "&= p(s'|s, a)\\pi(a'|s')\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mfrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
